{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JLF53mXc1YE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# ***Preprocessing***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4IoLVqsc4OV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=list(open(\"train.txt\",\"r\"))\n",
        "\n",
        "def preprocess(data):\n",
        "  tweets=[]\n",
        "  sents=[]\n",
        "  arr=[]\n",
        "  c=0\n",
        "  for i in data:\n",
        "    if(i==\"\\n\"):\n",
        "      tweets.append(arr)\n",
        "      arr=[]\n",
        "    else:\n",
        "      temp=i.split(\"\\t\")\n",
        "      if(len(temp)==3):\n",
        "        sents.append(temp[2])\n",
        "      arr.append(temp[0])\n",
        "  tweet=[]\n",
        "  for i in tweets:\n",
        "    temp=\"\"\n",
        "    for j in i:\n",
        "      temp+=j\n",
        "      temp+=\" \"\n",
        "    tweet.append(temp[:-1])\n",
        "  tweets=tweet \n",
        "  sent=[]\n",
        "  for i in sents:\n",
        "    temp=i[:-1]\n",
        "    if(temp==\"neutral\"):\n",
        "      sent.append(0)\n",
        "    elif(temp==\"positive\"):\n",
        "      sent.append(1)\n",
        "    else:\n",
        "      sent.append(2)  \n",
        "\n",
        "  return(tweets,sent)\n",
        "tweets,sent=preprocess(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muie_SL6iokm",
        "colab_type": "code",
        "outputId": "55ad04b3-ebfe-4e3a-d413-d288dd86bbb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df['tweet'] = tweets\n",
        "df['sent'] = sent\n",
        "print(df)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   tweet  sent\n",
            "0      meta @ AdilNisarButt pakistan ka ghra tauq he ...     2\n",
            "1      meta Madarchod mulle ye mathura me Nahi dikha ...     2\n",
            "2      meta @ narendramodi Manya Pradhan Mantri mahod...     1\n",
            "3      meta @ Atheist _ Krishna Jcb full trend me cha...     1\n",
            "4      meta @ AbhisharSharma _ @ RavishKumarBlog Loks...     1\n",
            "...                                                  ...   ...\n",
            "15126  meta @ rohitsharmawpg @ asadowaisi @ narendram...     2\n",
            "15127  meta @ Prof _ Hariom @ JKgrievance Who is BIJL...     2\n",
            "15128  meta @ amjedmbt @ bandisanjay _ bjp @ cpkarimn...     2\n",
            "15129  meta @ Sunju _ Mishra To phir bjp ke leader vi...     2\n",
            "15130  meta @ kunalkamra88 @ Swamy39 ISS ko @ BJP4Ind...     2\n",
            "\n",
            "[15131 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBG2k1GSiMKp",
        "colab_type": "code",
        "outputId": "dafe2cb1-c3d3-4d69-ded4-891bc8af7814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "data = open(\"test.txt\",\"r\")\n",
        "data=list(data)\n",
        "import pandas as pd\n",
        "tweets,sent=preprocess(data)\n",
        "test_df = pd.DataFrame()\n",
        "test_df['tweet'] = tweets\n",
        "test_df['sent'] = sent\n",
        "print(test_df)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  tweet  sent\n",
            "0     meta RT @ UAAPconfessions Love looks good on M...     0\n",
            "1     meta Ye Ye ..... ye ??????? We gonna start ano...     0\n",
            "2     meta @ zWffFY9JGklElA1 @ Min _ Of _ Lyching @ ...     0\n",
            "3     meta ~ Caring . ~ Bohot Jyada Caring . ~ Couri...     2\n",
            "4     meta @ AliHZaidiPTI @ SarfarazA _ 54 What none...     1\n",
            "...                                                 ...   ...\n",
            "1864  meta @ witchnextdoor _ Kahan ?? Itniiiiiiiiiii...     0\n",
            "1865  meta @ AmnaKhanPMLN 1992 ma Duain kabool hui t...     1\n",
            "1866  meta @ javerias Jo harami video bna rha he ye ...     0\n",
            "1867  meta RT @ falasizz FUCK MS THORPE FUCK MS BAAJ...     2\n",
            "1868  meta @ jigneshmevani80 @ dgpgujarat @ Ahmedaba...     2\n",
            "\n",
            "[1869 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEsKrYNigNVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def tokenize(sentence):\n",
        "    tokens = []\n",
        "    for t in re.findall(\"[a-zA-Z]+\",sentence.lower()):\n",
        "        if len(t)>=2:\n",
        "            tokens.append(t)\n",
        "    return tokens\n",
        "\n",
        "temp = []\n",
        "for i in df['tweet']:\n",
        "\ttemp.append(tokenize(i))\n",
        "print(temp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R17AAhXg_jYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp2 = []\n",
        "for i in temp:\n",
        "\ttemp2.append(\" \".join(i))\n",
        "print(temp2)\n",
        "df['tweet'] = temp2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpOFOv5u_qb6",
        "colab_type": "code",
        "outputId": "cc96e537-5d22-4114-8048-3ac3b0784f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UdQPz17-5bU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_D0dtRYAKbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open(\"hindi.txt\",\"r\")\n",
        "data=list(data)\n",
        "# print(data)\n",
        "for i in range(len(data)):\n",
        "  data[i] = data[i][:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS-2hvVqAbBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbwPuI5ldGYB",
        "colab_type": "code",
        "outputId": "98bf76e8-4f8e-48ad-8e9d-b85040ebc9dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "\n",
        "def remove_punct(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(lambda text: remove_punct(text))\n",
        "test_df[\"tweet\"] = test_df[\"tweet\"].apply(lambda text: remove_punct(text))\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([i for word in str(i).split() if i not in stops])\n",
        "\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(lambda text: remove_stopwords(text))\n",
        "test_df[\"tweet\"] = test_df[\"tweet\"].apply(lambda text: remove_stopwords(text))\n",
        "\n",
        "def removeLinks(df):\n",
        "    for i in range(len(df['tweet'])):\n",
        "        i_arr = df['tweet'][i].split(\" \")\n",
        "        if 'https' in i_arr:\n",
        "            ind = i_arr.index('https')\n",
        "            df['tweet'][i] = \" \".join(i_arr[:ind])\n",
        "    return df\n",
        "\n",
        "df = removeLinks(df)\n",
        "test_df = removeLinks(test_df)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFuCBcN6BDya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPdMazwHf1Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = []\n",
        "for i in df['tweet']:\n",
        "  temp.append(i.lower()[5:])\n",
        "df['tweet'] = temp\n",
        "\n",
        "temp = []\n",
        "for i in test_df['tweet']:\n",
        "  temp.append(i.lower()[5:])\n",
        "test_df['tweet'] = temp\n",
        "\n",
        "print(df)\n",
        "print(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_d_U8iDZhft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqilizSCZhkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2sU__WOZhoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRhC_vCjZhis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7B9JC1wZhdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51NpJl5osx7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "special = 'の誕生日当限定مئیوتکبćで、イベトステージليةاقدرがついに❇🇪ঈদےںسہమనకిద్ధతుشరా猫ねこబోయేఇసఅ한국댄스러시팀약자제일은행లఉండపషూవచ´غطنخผ่านไปแล้วครึงีขอบุณทกยูัزমোবারক°úլɩɣєɲ¥ਹੁਣਤਸੀਂਨਕਰੋਗੇਾਫਿ―➡ì🇨이나경開放すべてパズ멀티뷰와함께즐기는머터실황생중계만다립니ルをクリアるとムビまỉườ​⇒😦🍃™ī📈🔯ē⚕🎗はダヒョン🦅🌘📣⤴модкарт🇾ص🌌👒🥇⚘💍🥴î🤮'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwblNtDVf1cC",
        "colab_type": "code",
        "outputId": "8d6819e0-53d1-437f-d2ca-c787fa2b5b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "alphabet = ''\n",
        "for i in df['tweet']:\n",
        "  for j in i:\n",
        "    if(j not in alphabet):\n",
        "      if(j not in special):\n",
        "        alphabet += j\n",
        "print(alphabet)\n",
        "print(len(alphabet))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adilnsrbut pkghqeomcyjvfwzx\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JrXYRzff1fi",
        "colab_type": "code",
        "outputId": "c620ab6f-4304-43b4-9652-754643ebb7ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = 0\n",
        "for i in df['tweet']:\n",
        "  x = max(x,len(i))\n",
        "print(x)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8B4UON6f1hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_texts = [s.lower() for s in df['tweet']]\n",
        "test_texts = [s.lower() for s in test_df['tweet']]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "    \n",
        "tokenizer.word_index = char_dict.copy() \n",
        "tokenizer.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "\n",
        "y_train = df['sent']\n",
        "y_test =test_df['sent']\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_texts = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "train_data = pad_sequences(train_sequences, maxlen=137, padding='post')\n",
        "test_data = pad_sequences(test_texts, maxlen=137, padding='post')\n",
        "train_data = train_data, dtype='float32')\n",
        "test_data = np.array(test_data, dtype='float32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV1GlS1jf1k7",
        "colab_type": "code",
        "outputId": "a300a459-e6dd-4c2b-fdb1-2c59d644a4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(tk.word_index)\n",
        "vocab_size = len(tk.word_index)\n",
        "vocab_size"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 1, 'd': 2, 'i': 3, 'l': 4, 'n': 5, 's': 6, 'r': 7, 'b': 8, 'u': 9, 't': 10, ' ': 11, 'p': 12, 'k': 13, 'g': 14, 'h': 15, 'q': 16, 'e': 17, 'o': 18, 'm': 19, 'c': 20, 'y': 21, 'j': 22, 'v': 23, 'f': 24, 'w': 25, 'z': 26, 'x': 27, 'UNK': 28}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-ud_LB9f1nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_weights = []\n",
        "embedding_weights.append(np.zeros(vocab_size))\n",
        "for char, i in tk.word_index.items():\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i-1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "embedding_weights = np.array(embedding_weights)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZRnl32n6KBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TEPRdcf6QkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyBUSTujf1rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "dropout_p = 0.3\n",
        "optimizer = 'adam'\n",
        "loss = 'categorical_crossentropy'\n",
        "embedding_layer = Embedding(29, \n",
        "                            28,\n",
        "                            input_length=137,\n",
        "                            weights=[embedding_weights])\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnXCYBLgu9LI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln9FFRhjyyz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV9Ww5o-yhC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC448V4fu9Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faYhhsTpDn9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hBYbz0Ju9SM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNnRoXry4QYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "\n",
        "class AttLayer(Layer):\n",
        "    def __init__(self, attention_dim):\n",
        "        self.init = initializers.get('normal')\n",
        "        self.supports_masking = True\n",
        "        self.attention_dim = attention_dim\n",
        "        super(AttLayer, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)), name='W')\n",
        "        self.b = K.variable(self.init((self.attention_dim, )), name='b')\n",
        "        self.u = K.variable(self.init((self.attention_dim, 1)), name='u')\n",
        "        self.trainable_weights = [self.W, self.b, self.u]\n",
        "        super(AttLayer, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
        "        ait = K.dot(uit, self.u)\n",
        "        ait = K.squeeze(ait, -1)\n",
        "        ait = K.exp(ait)\n",
        "        if mask is not None:\n",
        "            ait *= K.cast(mask, K.floatx())\n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = x * ait\n",
        "        output = K.sum(weighted_input, axis=1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP_aKV0Vu9NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import TimeDistributed\n",
        "from keras import Sequential\n",
        "from keras import initializers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7VN1bwqu9JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_binary = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "x_test = test_data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(train_data,y_binary,test_size=0.15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97k5C1ZdJwe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6q9DmceJwjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tfidf_train, x_val, y_train, y_val = train_test_split(Train_X_Tfidf,y_binary,test_size=0.15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KssXVWy3JwnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5faf1a0b-7b0f-4351-ded6-e36fcb8ed25b"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12861, 137)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-WZbbUUJwhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHj5ylXQg8Vi",
        "colab_type": "text"
      },
      "source": [
        "# ***Conv LSTM with Attention***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St1lswWAJwcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0kIHQIi-Qv4",
        "colab_type": "code",
        "outputId": "c2971c50-391f-404d-9f79-4639c2b926c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "source": [
        "from keras.layers import SpatialDropout1D\n",
        "from keras.layers import GRU\n",
        "inp = Input(shape=(137,))  \n",
        "x = embedding_layer(inp)\n",
        "\n",
        "x = Conv1D(512, 8)(x) \n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(pool_size=4)(x)\n",
        "x = Conv1D(256, 8)(x) \n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(pool_size=4)(x)\n",
        "x = LSTM(128, return_sequences=True, dropout=0.2,\n",
        "                           recurrent_dropout=0.25)(x)\n",
        "x = LSTM(64, return_sequences=True)(x)\n",
        "att =  AttLayer(128)(x)\n",
        "x = Dense(64, activation=\"relu\")(att)\n",
        "preds = Dense(3, activation='softmax')(x)\n",
        "#output = Activation('relu')(output)\n",
        "#prediction = Dense(1, activation = \"sigmoid\")(conc)\n",
        "model = Model(inputs = inp, outputs = preds)\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\",f1_m,precision_m, recall_m])\n",
        "model.fit(x_train, y_train,\n",
        "          validation_data=(x_val, y_val),\n",
        "          batch_size=32,\n",
        "          epochs=17,\n",
        "          verbose=2)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12861 samples, validate on 2270 samples\n",
            "Epoch 1/17\n",
            " - 24s - loss: 1.0067 - acc: 0.4675 - f1_m: 0.2270 - precision_m: 0.4309 - recall_m: 0.1642 - val_loss: 1.0062 - val_acc: 0.5048 - val_f1_m: 0.3475 - val_precision_m: 0.6089 - val_recall_m: 0.2458\n",
            "Epoch 2/17\n",
            " - 16s - loss: 0.8776 - acc: 0.5745 - f1_m: 0.4910 - precision_m: 0.6454 - recall_m: 0.4040 - val_loss: 0.8661 - val_acc: 0.5819 - val_f1_m: 0.5437 - val_precision_m: 0.6270 - val_recall_m: 0.4815\n",
            "Epoch 3/17\n",
            " - 16s - loss: 0.7829 - acc: 0.6408 - f1_m: 0.6095 - precision_m: 0.6761 - recall_m: 0.5571 - val_loss: 0.8996 - val_acc: 0.5793 - val_f1_m: 0.5590 - val_precision_m: 0.6062 - val_recall_m: 0.5198\n",
            "Epoch 4/17\n",
            " - 16s - loss: 0.6858 - acc: 0.7096 - f1_m: 0.6993 - precision_m: 0.7304 - recall_m: 0.6719 - val_loss: 1.0372 - val_acc: 0.5573 - val_f1_m: 0.5489 - val_precision_m: 0.5603 - val_recall_m: 0.5383\n",
            "Epoch 5/17\n",
            " - 16s - loss: 0.5478 - acc: 0.7805 - f1_m: 0.7779 - precision_m: 0.7946 - recall_m: 0.7625 - val_loss: 0.9856 - val_acc: 0.5833 - val_f1_m: 0.5797 - val_precision_m: 0.5935 - val_recall_m: 0.5670\n",
            "Epoch 6/17\n",
            " - 16s - loss: 0.3796 - acc: 0.8635 - f1_m: 0.8640 - precision_m: 0.8715 - recall_m: 0.8570 - val_loss: 1.1432 - val_acc: 0.5731 - val_f1_m: 0.5714 - val_precision_m: 0.5783 - val_recall_m: 0.5648\n",
            "Epoch 7/17\n",
            " - 16s - loss: 0.2689 - acc: 0.9103 - f1_m: 0.9100 - precision_m: 0.9144 - recall_m: 0.9058 - val_loss: 1.3716 - val_acc: 0.5683 - val_f1_m: 0.5654 - val_precision_m: 0.5698 - val_recall_m: 0.5612\n",
            "Epoch 8/17\n",
            " - 16s - loss: 0.1935 - acc: 0.9395 - f1_m: 0.9397 - precision_m: 0.9421 - recall_m: 0.9374 - val_loss: 1.4788 - val_acc: 0.5670 - val_f1_m: 0.5669 - val_precision_m: 0.5723 - val_recall_m: 0.5617\n",
            "Epoch 9/17\n",
            " - 16s - loss: 0.1633 - acc: 0.9482 - f1_m: 0.9481 - precision_m: 0.9501 - recall_m: 0.9462 - val_loss: 1.5115 - val_acc: 0.5661 - val_f1_m: 0.5612 - val_precision_m: 0.5671 - val_recall_m: 0.5555\n",
            "Epoch 10/17\n",
            " - 16s - loss: 0.1409 - acc: 0.9551 - f1_m: 0.9552 - precision_m: 0.9562 - recall_m: 0.9542 - val_loss: 1.7710 - val_acc: 0.5722 - val_f1_m: 0.5724 - val_precision_m: 0.5744 - val_recall_m: 0.5705\n",
            "Epoch 11/17\n",
            " - 16s - loss: 0.1166 - acc: 0.9642 - f1_m: 0.9643 - precision_m: 0.9650 - recall_m: 0.9635 - val_loss: 1.5907 - val_acc: 0.5643 - val_f1_m: 0.5645 - val_precision_m: 0.5684 - val_recall_m: 0.5608\n",
            "Epoch 12/17\n",
            " - 16s - loss: 0.0927 - acc: 0.9702 - f1_m: 0.9704 - precision_m: 0.9715 - recall_m: 0.9694 - val_loss: 1.8697 - val_acc: 0.5626 - val_f1_m: 0.5611 - val_precision_m: 0.5638 - val_recall_m: 0.5586\n",
            "Epoch 13/17\n",
            " - 16s - loss: 0.0818 - acc: 0.9701 - f1_m: 0.9703 - precision_m: 0.9712 - recall_m: 0.9694 - val_loss: 1.8534 - val_acc: 0.5727 - val_f1_m: 0.5720 - val_precision_m: 0.5744 - val_recall_m: 0.5696\n",
            "Epoch 14/17\n",
            " - 16s - loss: 0.0700 - acc: 0.9747 - f1_m: 0.9750 - precision_m: 0.9759 - recall_m: 0.9743 - val_loss: 1.8564 - val_acc: 0.5608 - val_f1_m: 0.5600 - val_precision_m: 0.5615 - val_recall_m: 0.5586\n",
            "Epoch 15/17\n",
            " - 16s - loss: 0.0711 - acc: 0.9750 - f1_m: 0.9750 - precision_m: 0.9765 - recall_m: 0.9735 - val_loss: 2.0819 - val_acc: 0.5604 - val_f1_m: 0.5600 - val_precision_m: 0.5620 - val_recall_m: 0.5581\n",
            "Epoch 16/17\n",
            " - 16s - loss: 0.0711 - acc: 0.9747 - f1_m: 0.9744 - precision_m: 0.9759 - recall_m: 0.9730 - val_loss: 1.8032 - val_acc: 0.5687 - val_f1_m: 0.5682 - val_precision_m: 0.5719 - val_recall_m: 0.5648\n",
            "Epoch 17/17\n",
            " - 16s - loss: 0.0583 - acc: 0.9775 - f1_m: 0.9776 - precision_m: 0.9786 - recall_m: 0.9767 - val_loss: 2.0917 - val_acc: 0.5709 - val_f1_m: 0.5712 - val_precision_m: 0.5728 - val_recall_m: 0.5696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd96048a470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTrnATW0-Q6U",
        "colab_type": "code",
        "outputId": "7dc3203d-8684-41de-9b46-58ba14707fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1869/1869 [==============================] - 1s 444us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.485415195002793,\n",
              " 0.5238095238733061,\n",
              " 0.5227878668079917,\n",
              " 0.5250602644225887,\n",
              " 0.520599251000112]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIFJzQmwjAiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_deep = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCPanhI9-RCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "867aa1d1-b943-4b8a-e672-64ffb1f98418"
      },
      "source": [
        "y_pred_bool = np.argmax(y_pred_deep, axis=1)\n",
        "y_test_bool = np.argmax(y_test, axis=1)\n",
        "print(classification_report(y_pred_bool, y_test_bool))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.52      0.48       664\n",
            "           1       0.57      0.54      0.55       621\n",
            "           2       0.57      0.52      0.54       584\n",
            "\n",
            "    accuracy                           0.52      1869\n",
            "   macro avg       0.53      0.52      0.53      1869\n",
            "weighted avg       0.53      0.52      0.53      1869\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F38jwqfRjsTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "436b6926-f37e-475e-dc72-d1d24c756e9b"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajow8s0ddVEm",
        "colab_type": "text"
      },
      "source": [
        "# ***Best  obtained ML Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ori7XVp5-RFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "\n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(df['tweet'])\n",
        "\n",
        "Train_X_Tfidf = Tfidf_vect.transform(df['tweet'])\n",
        "Test_X_Tfidf = Tfidf_vect.transform(test_df['tweet'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkHSRyOs-RAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,df['sent'])\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhxlR7_1-Q-j",
        "colab_type": "code",
        "outputId": "ae1895cf-4864-47bb-f1e1-6dccc6a83f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_df['sent'])*100)\n",
        "print(classification_report(predictions_SVM, test_df['sent']))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Accuracy Score ->  55.48421615837347\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.52      0.53       771\n",
            "           1       0.57      0.57      0.57       577\n",
            "           2       0.57      0.58      0.57       521\n",
            "\n",
            "    accuracy                           0.55      1869\n",
            "   macro avg       0.56      0.56      0.56      1869\n",
            "weighted avg       0.55      0.55      0.55      1869\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XR6SKdRj65D",
        "colab_type": "code",
        "outputId": "fac6ce38-2075-4787-dbe9-d797dcee9135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "predictions_SVM[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 2, 2, 1, 1, 0, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 2, 0,\n",
              "       0, 1, 2, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 2, 1, 2,\n",
              "       2, 1, 1, 1, 0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 0, 0, 2, 0, 1, 0, 1, 1,\n",
              "       0, 0, 1, 2, 0, 2, 1, 2, 1, 1, 0, 0, 2, 0, 2, 1, 1, 0, 1, 0, 2, 0,\n",
              "       1, 2, 2, 1, 2, 1, 2, 0, 0, 0, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QV-GWAF-Q42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkLm5fy_UsvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDlcDQxoV9Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEAj2193V9rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}